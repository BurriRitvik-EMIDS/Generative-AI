import os
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_classic.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.messages import HumanMessage, AIMessage
from langchain_classic.chains import LLMChain
from langchain_core.prompts import PromptTemplate
import gr


# Secure API Key
if "GOOGLE_API_KEY" in st.secrets:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
else:
    st.error("Missing GOOGLE_API_KEY in Streamlit secrets.")
    st.stop()

# Constants
PDF_FILE = "who_tb_diabetes.pdf"
APP_TITLE = "Health Report Q&A Bot"
APP_SUBTITLE = "WHO TB-Diabetes Report Analysis"

# G-Eval Prompt Template
GEVAL_PROMPT_TEMPLATE = """
You are a healthcare evaluation expert specializing in tuberculosis and diabetes comorbidity.

Your task is to assess the quality, reliability, and usefulness of a response generated by an AI assistant, 
which is grounded in the official WHO report titled:

"Collaborative Framework for Care and Control of Tuberculosis and Diabetes."

---

Use the following WHO-derived context: 
{context}

User's Question: 
{question}

AI Assistant's Response: 
{response}

---

### Evaluation Task:

Evaluate the response across the four key clinical dimensions. 
Use a score from 1 (poor) to 5 (excellent) and provide brief, clear justifications for each.

### Evaluation Criteria:

1. **Factual Accuracy**  
   - Does the response contain medically accurate and factually correct content derived from the WHO report?  
   - Penalize heavily if it introduces hallucinated facts, unsupported recommendations, or misinformation.

2. **Medical Relevance**  
   - How well does the response align with the clinical or public health need behind the user's question?  
   - Penalize if the response is too generic, off-topic, or unrelated to TB-diabetes care.

3. **Evidence Grounding**  
   - Does the response appear to be clearly supported by the document context (either directly cited or reasonably inferred)?  
   - Reward when the response anchors decisions or insights in highly grounded, document-faithful portions of the WHO report.

4. **Role-Based Clarity**  
   - Is the tone, terminology, and depth appropriate for the user's role (e.g., Doctor, Patient, Policy Maker, Researcher)?  
   - Penalize if the explanation is too technical for a patient or too simplistic for a Doctor.

---

### Dynamic Role Prioritization Guidelines:

Confidence Score = Î£ (metric Ã— weight) Ã— 20  
(Factual Accuracy Ã— 0.4 + Medical Relevance Ã— 0.25 + Evidence Grounding Ã— 0.25 + Role-Based Clarity Ã— 0.1) Ã— 20

Use this formula as a base, and dynamically adjust the weights if the user's role demands a different prioritization.

---

### Output Format:

Your response must be formatted as markdown text:

#### 1. Factual Accuracy  
**Score:** 5 / 5  
**Explanation:** ...  

#### 2. Medical Relevance  
**Score:** 4 / 5  
**Explanation:** ...

#### 3. Evidence Grounding  
**Score:** 5 / 5  
**Explanation:** ...

#### 4. Role-Based Clarity  
**Score:** 4 / 5  
**Explanation:** ...

---

### Confidence Score: 96 / 100

Ensure your response is strictly in this format.
Ensure role-based expectations are reflected in your scores and reasoning.
Be strict, fair, and clinically responsible in your judgment.
"""

# Suggestions Prompt Template
SUGGESTIONS_PROMPT_TEMPLATE = """
Based on the WHO TB-Diabetes report and the current conversation context, suggest exactly 3 follow-up questions that would be most valuable for a {role} ({category}) to ask.

Context from the conversation:
Previous Question: {question}
Previous Response: {response}

Generate 3 specific, actionable questions that:
1. Are relevant to the {role}'s professional needs
2. Build upon the previous response
3. Would help deepen understanding of TB-Diabetes topics

Format your response as a numbered list:
1. [First question]
2. [Second question]
3. [Third question]

Only output the 3 questions, nothing else.
"""

# ðŸ§  1. Load & Embed (Updated Model)
@st.cache_resource
def load_faiss_index():
    if not os.path.exists(PDF_FILE):
        return None
    
    loader = PyPDFLoader(PDF_FILE)
    pages = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs = splitter.split_documents(pages)
    
    # âœ… IMPROVEMENT: Use the modern, efficient embedding model
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    return FAISS.from_documents(docs, embeddings)

# ðŸ”— 2. Modern LCEL Chain
def get_rag_chain(vectorstore):
    llm = ChatGoogleGenerativeAI(
        model="gemini-3-flash-preview", 
        temperature=0.3
    )
    
    retriever = vectorstore.as_retriever()

    # A. History-Aware Retriever (Rephrases query based on history)
    history_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )
    
    history_prompt = ChatPromptTemplate.from_messages([
        ("system", history_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, history_prompt
    )

    # B. Answer Generation Chain
    system_prompt = (
        "You are a domain-specific assistant grounded in the WHO TBâ€“Diabetes report.\n\n"
        "The user belongs to the category **{category}**, and their role is **{role}**.\n"
        "Based on this role, tailor your response using the appropriate tone, depth, and terminology "
        "that best serves the user's professional context and information needs.\n\n"
        "Use only facts derived from the WHO document.\n\n"
        "Context from WHO Report:\n"
        "{context}\n\n"
        "Your response must be:\n"
        "- Consistent with the WHO's findings and guidelines.\n"
        "- Aligned with the role's expectations (e.g., clinical detail for doctors, practical steps for caregivers, policy-level summaries for administrators).\n"
        "- Clear, concise, and relevant to the user's domain.\n\n"
        "Respond accordingly, ensuring domain-specific accuracy and usefulness."
    )
    
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    
    # C. Final Retrieval Chain
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain

# G-Eval LLM Chain
@st.cache_resource
def build_eval_chain():
    eval_prompt = PromptTemplate(
        input_variables=["context", "question", "response"],
        template=GEVAL_PROMPT_TEMPLATE
    )
    llm = ChatGoogleGenerativeAI(model="gemini-3-flash-preview", temperature=0)
    return LLMChain(prompt=eval_prompt, llm=llm)

# Suggestions LLM Chain
@st.cache_resource
def build_suggestions_chain():
    suggestions_prompt = PromptTemplate(
        input_variables=["role", "category", "question", "response"],
        template=SUGGESTIONS_PROMPT_TEMPLATE
    )
    llm = ChatGoogleGenerativeAI(model="gemini-3-flash-preview", temperature=0.7)
    return LLMChain(prompt=suggestions_prompt, llm=llm)

# Page Configuration
st.set_page_config(
    page_title=APP_TITLE,
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for Production-Grade Dark Mode UI
st.markdown("""
<style>
    /* Main container styling */
    .main {
        background: linear-gradient(135deg, #0f0f0f 0%, #1a1a1a 100%);
    }
    
    /* Header styling */
    .app-header {
        text-align: center;
        padding: 2rem 0 1rem 0;
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        border-radius: 12px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
    }
    
    .app-title {
        font-size: 2.5rem;
        font-weight: 700;
        color: #ffffff;
        margin: 0;
        letter-spacing: -0.5px;
    }
    
    .app-subtitle {
        font-size: 1.1rem;
        color: #b8d4ff;
        margin-top: 0.5rem;
        font-weight: 400;
    }
    
    /* Sidebar styling */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg, #1a1a1a 0%, #0f0f0f 100%);
        border-right: 1px solid #2d2d2d;
    }
    
    [data-testid="stSidebar"] .element-container {
        color: #e0e0e0;
    }
    
    /* Message bubbles */
    .user-message {
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        color: #ffffff;
        padding: 1.2rem;
        border-radius: 12px;
        margin-bottom: 1rem;
        border-left: 4px solid #4a90e2;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
    }
    
    .assistant-message {
        background: linear-gradient(135deg, #1a3a1a 0%, #2d5a2d 100%);
        color: #ffffff;
        padding: 1.2rem;
        border-radius: 12px;
        margin-bottom: 1rem;
        border-left: 4px solid #4caf50;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
    }
    
    .message-label {
        font-weight: 600;
        font-size: 0.9rem;
        margin-bottom: 0.5rem;
        opacity: 0.9;
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }
    
    .message-content {
        font-size: 1rem;
        line-height: 1.6;
    }
    
    /* Input styling */
    .stTextInput input {
        background-color: #1a1a1a;
        color: #ffffff;
        border: 1px solid #3d3d3d;
        border-radius: 8px;
        padding: 0.75rem;
        font-size: 1rem;
    }
    
    .stTextInput input:focus {
        border-color: #4a90e2;
        box-shadow: 0 0 0 2px rgba(74, 144, 226, 0.2);
    }
    
    /* Button styling */
    .stButton button {
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.6rem 1.5rem;
        font-weight: 600;
        transition: all 0.3s ease;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
    }
    
    .stButton button:hover {
        background: linear-gradient(135deg, #2a5298 0%, #1e3c72 100%);
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
        transform: translateY(-1px);
    }
    
    /* Select box styling */
    .stSelectbox select {
        background-color: #1a1a1a;
        color: #ffffff;
        border: 1px solid #3d3d3d;
        border-radius: 8px;
    }
    
    /* Expander styling */
    .streamlit-expanderHeader {
        background-color: #1a1a1a;
        color: #ffffff;
        border-radius: 8px;
        border: 1px solid #3d3d3d;
    }
    
    .streamlit-expanderContent {
        background-color: #0f0f0f;
        border: 1px solid #2d2d2d;
        border-radius: 0 0 8px 8px;
    }
    
    /* Warning and info boxes */
    .stWarning, .stInfo {
        background-color: #1a1a1a;
        border-left: 4px solid #ff9800;
        border-radius: 8px;
        color: #ffffff;
    }
    
    /* Hide Streamlit branding */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    
    /* Divider */
    hr {
        border: none;
        border-top: 1px solid #2d2d2d;
        margin: 2rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Header
st.markdown(f"""
<div class="app-header">
    <h1 class="app-title">{APP_TITLE}</h1>
    <p class="app-subtitle">{APP_SUBTITLE}</p>
</div>
""", unsafe_allow_html=True)

# Sidebar Configuration
with st.sidebar:
    st.markdown("### User Profile")
    
    roles_map = {
        "Healthcare Professional": ["Doctor", "Physician", "Nurse", "Pharmacist", "Lab Technician"],
        "Student or Trainee": ["Medical Student", "Public Health Student", "Intern"],
        "General Public": ["Patient", "Caregiver", "At-Risk Individual"],
        "Policy / System Worker": ["Health Admin", "Policy Maker", "NGO Worker"],
        "Research & Data": ["Researcher", "Epidemiologist", "Public Health Analyst"]
    }

    category_options = ["Select a Category"] + list(roles_map.keys())
    category = st.selectbox("Category", category_options, key="category_select")

    if category != "Select a Category":
        role_options = ["Select a Role"] + roles_map[category]
        role = st.selectbox("Role", role_options, key="role_select")
    else:
        role = None

    st.markdown("---")
    st.caption("The assistant tailors responses based on your selected role.")
    
    # Clear current role's chat only
    if st.button("Clear Current Chat", use_container_width=True):
        st.session_state.messages = []
        st.session_state.chat_history = []
        # Also clear from role_chats storage
        if role and category:
            current_key = f"{category}|{role}"
            if current_key in st.session_state.get("role_chats", {}):
                del st.session_state.role_chats[current_key]
        st.rerun()
    
    # Clear ALL roles' chats
    if st.button("Clear All Chats", use_container_width=True):
        st.session_state.messages = []
        st.session_state.chat_history = []
        st.session_state.role_chats = {}
        st.rerun()

# Initialize Session State
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "messages" not in st.session_state:
    st.session_state.messages = []

if "eval_chain" not in st.session_state:
    st.session_state.eval_chain = build_eval_chain()

if "suggestions_chain" not in st.session_state:
    st.session_state.suggestions_chain = build_suggestions_chain()

# Role-specific chat storage (persists chats per role)
if "role_chats" not in st.session_state:
    st.session_state.role_chats = {}  # key: "category|role", value: {"messages": [], "chat_history": []}

if "current_role" not in st.session_state:
    st.session_state.current_role = None

if "current_category" not in st.session_state:
    st.session_state.current_category = None

# Handle role/category changes - save current and load previous
if role and category and role != "Select a Role" and category != "Select a Category":
    current_key = f"{category}|{role}"
    previous_key = f"{st.session_state.current_category}|{st.session_state.current_role}" if st.session_state.current_role else None
    
    if st.session_state.current_role != role or st.session_state.current_category != category:
        # Save current chat to previous role (if exists)
        if previous_key and st.session_state.current_role:
            st.session_state.role_chats[previous_key] = {
                "messages": st.session_state.messages.copy(),
                "chat_history": st.session_state.chat_history.copy()
            }
        
        # Load chat for new role (or start fresh)
        if current_key in st.session_state.role_chats:
            # Restore previous conversation for this role
            saved = st.session_state.role_chats[current_key]
            st.session_state.messages = saved["messages"].copy()
            st.session_state.chat_history = saved["chat_history"].copy()
        else:
            # New role - start fresh
            st.session_state.messages = []
            st.session_state.chat_history = []
        
        # Update current role tracking
        st.session_state.current_role = role
        st.session_state.current_category = category

# Load Vector Store
vectorstore = load_faiss_index()

if not vectorstore:
    st.error("PDF file not found. Please check the path to 'who_tb_diabetes.pdf'.")
    st.stop()

# Role Validation
if not role or role == "Select a Role" or not category or category == "Select a Category":
    st.warning("Please select both a Category and a Role from the sidebar before asking questions.")
    st.info("Use the sidebar to choose your professional profile first.")
else:
    # Build RAG Chain
    chain = get_rag_chain(vectorstore)
    
    # Initialize Guardrails
    guard = gr.Guard()
    
    # Display conversation history using native chat components
    for msg_data in st.session_state.messages:
        # Handle both old (3-tuple) and new (4-tuple) message formats
        if len(msg_data) == 4:
            sender, msg, eval_text, suggestions = msg_data
        else:
            sender, msg, eval_text = msg_data
            suggestions = None
            
        if sender == "user":
            with st.chat_message("user"):
                st.markdown(f"**{role}**")
                st.write(msg)
        elif sender == "bot":
            with st.chat_message("assistant"):
                st.write(msg)
                if eval_text:
                    with st.expander("View Response Evaluation"):
                        st.markdown(eval_text)
                if suggestions:
                    with st.expander("Suggested Follow-up Questions"):
                        st.markdown(suggestions)
    
    # Native chat input (fixed at bottom, Enter to send)
    user_input = st.chat_input("Type your question here...")
    
    if user_input:
        # Validate input
        input_error = guard.validate_input(user_input)
        if input_error:
            st.warning(f"Input Validation Error: {input_error}")
        else:
            # Display user message immediately
            with st.chat_message("user"):
                st.markdown(f"**{role}**")
                st.write(user_input)
            
            # Display assistant response with loading
            with st.chat_message("assistant"):
                with st.spinner("Thinking..."):
                    try:
                        # Generate response
                        response = chain.invoke({
                            "input": user_input,
                            "chat_history": st.session_state.chat_history,
                            "role": role,
                            "category": category
                        })
                        
                        bot_answer = response["answer"]
                        context_docs = response.get("context", [])
                        
                        # Extract context text for evaluation
                        if isinstance(context_docs, list):
                            context_text = "\n\n".join([doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in context_docs])
                        else:
                            context_text = str(context_docs)
                        
                        # Output validation
                        output_error = guard.validate_response(bot_answer)
                        if output_error:
                            st.warning(f"Output Validation Warning: {output_error}")
                        
                        # Medical safety flags
                        safety_flags = guard.check_medical_safety_flags(bot_answer)
                        for flag in safety_flags:
                            st.warning(f"Medical Safety Alert: {flag}")
                        
                        # Display the answer
                        st.write(bot_answer)
                        
                        # G-Eval Assessment
                        eval_text = ""
                        try:
                            eval_response = st.session_state.eval_chain.invoke({
                                "context": context_text,
                                "question": user_input,
                                "response": bot_answer
                            })
                            eval_text = eval_response.get("text", "No evaluation returned.")
                        except Exception as eval_error:
                            eval_text = f"Evaluation failed: {str(eval_error)}"
                        
                        if eval_text:
                            with st.expander("View Response Evaluation"):
                                st.markdown(eval_text)
                        
                        # Generate Suggestions
                        suggestions_text = ""
                        try:
                            suggestions_response = st.session_state.suggestions_chain.invoke({
                                "role": role,
                                "category": category,
                                "question": user_input,
                                "response": bot_answer
                            })
                            suggestions_text = suggestions_response.get("text", "")
                        except Exception as sugg_error:
                            suggestions_text = f"Suggestions failed: {str(sugg_error)}"
                        
                        if suggestions_text:
                            with st.expander("Suggested Follow-up Questions"):
                                st.markdown(suggestions_text)
                        
                        # Update chat history
                        st.session_state.chat_history.append(HumanMessage(content=user_input))
                        st.session_state.chat_history.append(AIMessage(content=bot_answer))
                        
                        # Store in messages for display (4-tuple: sender, msg, eval, suggestions)
                        st.session_state.messages.append(("user", user_input, None, None))
                        st.session_state.messages.append(("bot", bot_answer, eval_text, suggestions_text))
                        
                    except Exception as e:
                        st.error(f"An error occurred: {str(e)}")
                        st.info("Please try rephrasing your question or check your API credentials.")


